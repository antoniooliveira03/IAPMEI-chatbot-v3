{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711c270-f8b7-47db-a01a-49176ade6961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158c077-0524-4fec-b7ca-f3fd7abc66c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c04985-fb96-4176-924a-87bd094a44af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:48:52.508276Z",
     "iopub.status.busy": "2025-12-24T09:48:52.507927Z",
     "iopub.status.idle": "2025-12-24T09:48:52.514905Z",
     "shell.execute_reply": "2025-12-24T09:48:52.513679Z",
     "shell.execute_reply.started": "2025-12-24T09:48:52.508244Z"
    }
   },
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def capture_endpoints():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        endpoints = set()\n",
    "\n",
    "        page.on(\n",
    "            \"response\",\n",
    "            lambda res: endpoints.add(res.url)\n",
    "            if \"/webapi/api/\" in res.url else None\n",
    "        )\n",
    "\n",
    "        await page.goto(\"https://financiamento.iapmei.pt/inicio/home\", wait_until=\"domcontentloaded\")\n",
    "\n",
    "        # ⬇️ THIS IS THE IMPORTANT PART\n",
    "        await page.wait_for_timeout(3000)\n",
    "        await page.mouse.wheel(0, 3000)   # trigger lazy loading\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        await browser.close()\n",
    "        return endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d1ed7-28b0-4968-8ccf-0135f0732105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1699557c-7a4b-45c7-9720-2da50ab07dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:48:53.647679Z",
     "iopub.status.busy": "2025-12-24T09:48:53.647389Z",
     "iopub.status.idle": "2025-12-24T09:49:02.852113Z",
     "shell.execute_reply": "2025-12-24T09:49:02.850286Z",
     "shell.execute_reply.started": "2025-12-24T09:48:53.647654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://financiamento.iapmei.pt/webapi/api/ItensPesquisas?caraterizacao=2\n",
      "https://financiamento.iapmei.pt/webapi/api/Paginas/Footer\n",
      "https://financiamento.iapmei.pt/webapi/api/indicadores\n",
      "https://financiamento.iapmei.pt/webapi/api/ItensPesquisas/arrayvalores\n",
      "https://financiamento.iapmei.pt/webapi/api/paginas\n",
      "https://financiamento.iapmei.pt/webapi/api/sliders\n",
      "https://financiamento.iapmei.pt/webapi/api/ItensPesquisas\n",
      "https://financiamento.iapmei.pt/webapi/api/ItensPesquisas?caraterizacao=1\n",
      "https://financiamento.iapmei.pt/webapi/api/Itens/arrayvalores\n",
      "https://financiamento.iapmei.pt/webapi/api/Configuracao/GetRedesSociais\n"
     ]
    }
   ],
   "source": [
    "endpoints = await capture_endpoints()\n",
    "for e in endpoints:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b71866c4-1a6c-44d4-9092-ec56353af38a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T09:58:09.809319Z",
     "iopub.status.busy": "2025-12-24T09:58:09.809028Z",
     "iopub.status.idle": "2025-12-24T09:58:21.295692Z",
     "shell.execute_reply": "2025-12-24T09:58:21.294639Z",
     "shell.execute_reply.started": "2025-12-24T09:58:09.809294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 10 API endpoints\n",
      "Processing 7 relevant endpoints\n",
      "Fetched https://financiamento.iapmei.pt/webapi/api/ItensPesquisas?caraterizacao=2 → 0 chars\n",
      "Fetched https://financiamento.iapmei.pt/webapi/api/Paginas/Footer → 7578 chars\n",
      "Fetched https://financiamento.iapmei.pt/webapi/api/ItensPesquisas/arrayvalores → 0 chars\n",
      "Fetched https://financiamento.iapmei.pt/webapi/api/paginas → 11209 chars\n",
      "Fetched https://financiamento.iapmei.pt/webapi/api/ItensPesquisas → 0 chars\n",
      "Fetched https://financiamento.iapmei.pt/webapi/api/ItensPesquisas?caraterizacao=1 → 0 chars\n",
      "Fetched https://financiamento.iapmei.pt/webapi/api/Itens/arrayvalores → 0 chars\n",
      "All content saved to data/financiamento_complete.json\n",
      "Total documents: 7\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\"data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://financiamento.iapmei.pt/webapi/api\"\n",
    "\n",
    "# ------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------\n",
    "def clean_html(html_str):\n",
    "    if not html_str:\n",
    "        return \"\"\n",
    "    return BeautifulSoup(html_str, \"html.parser\").get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "def fetch_json(url, params=None):\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def extract_text_from_data(data):\n",
    "    texts = []\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key in [\"titulo\", \"descricao\", \"conteudo\", \"texto\"]:\n",
    "            val = data.get(key)\n",
    "            if isinstance(val, str) and val.strip():\n",
    "                texts.append(clean_html(val))\n",
    "\n",
    "        # Rich text components\n",
    "        for comp in data.get(\"componentes\", []):\n",
    "            if isinstance(comp, dict):\n",
    "                for v in comp.values():\n",
    "                    if isinstance(v, str) and len(v.strip()) > 50:\n",
    "                        texts.append(clean_html(v))\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        for el in data:\n",
    "            if isinstance(el, dict):\n",
    "                for v in el.values():\n",
    "                    if isinstance(v, str) and len(v.strip()) > 50:\n",
    "                        texts.append(clean_html(v))\n",
    "            elif isinstance(el, str) and el.strip():\n",
    "                texts.append(clean_html(el))\n",
    "\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Capture endpoints via Playwright\n",
    "# ------------------------------\n",
    "async def capture_endpoints():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)  # set True to hide browser\n",
    "        page = await browser.new_page()\n",
    "        endpoints = set()\n",
    "\n",
    "        # Capture all API responses\n",
    "        page.on(\"response\", lambda res: endpoints.add(res.url) if \"/webapi/api/\" in res.url else None)\n",
    "\n",
    "        # Open homepage\n",
    "        await page.goto(\"https://financiamento.iapmei.pt/inicio/home\", wait_until=\"domcontentloaded\")\n",
    "\n",
    "        # Trigger lazy-loaded content\n",
    "        await page.wait_for_timeout(3000)\n",
    "        await page.mouse.wheel(0, 3000)\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        await browser.close()\n",
    "        return endpoints\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Fetch content from discovered endpoints\n",
    "# ------------------------------\n",
    "async def main():\n",
    "    endpoints = await capture_endpoints()\n",
    "    print(f\"Discovered {len(endpoints)} API endpoints\")\n",
    "    \n",
    "    dataset = []\n",
    "\n",
    "    # Filter relevant endpoints: only Itens / Paginas / ItensPesquisas\n",
    "    relevant = [e for e in endpoints if any(x in e.lower() for x in [\"/itens\", \"/paginas\", \"/itenspesquisas\"])]\n",
    "    print(f\"Processing {len(relevant)} relevant endpoints\")\n",
    "\n",
    "    for url in relevant:\n",
    "        try:\n",
    "            data = fetch_json(url)\n",
    "            text = extract_text_from_data(data)\n",
    "\n",
    "            dataset.append({\n",
    "                \"url\": url,\n",
    "                \"text\": text\n",
    "            })\n",
    "            print(f\"Fetched {url} → {len(text)} chars\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "    # Save final dataset\n",
    "    output_file = OUTPUT_DIR / \"financiamento_complete.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"All content saved to {output_file}\")\n",
    "    print(f\"Total documents: {len(dataset)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Run\n",
    "# ------------------------------\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# Instead of asyncio.run(main()), do:\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66dc18-71ea-480a-8bea-ef8028cc8a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "208dad7a-0987-4706-8faf-812e3070ddba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T10:19:10.270950Z",
     "iopub.status.busy": "2025-12-24T10:19:10.270615Z",
     "iopub.status.idle": "2025-12-24T10:19:17.707059Z",
     "shell.execute_reply": "2025-12-24T10:19:17.705859Z",
     "shell.execute_reply.started": "2025-12-24T10:19:10.270916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing site: https://financiamento.iapmei.pt/inicio/home\n",
      "\n",
      "Processing site: https://transparencia.gov.pt/\n",
      "  Discovered 0 JSON endpoints\n",
      "  Extracted DOM text → 208 chars\n",
      "  Discovered 10 JSON endpoints\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/ItensPesquisas?caraterizacao=2 → 4398 chars\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/Paginas/Footer → 7787 chars\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/indicadores → 92 chars\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/ItensPesquisas/arrayvalores → 133 chars\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/paginas → 11528 chars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/fxsq_1490x9dd2w76tqvt3kr0000gn/T/ipykernel_2691/3448370487.py:27: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  return BeautifulSoup(html_str, \"html.parser\").get_text(separator=\"\\n\", strip=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetched https://financiamento.iapmei.pt/webapi/api/sliders → 2064 chars\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/ItensPesquisas → 10095 chars\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/ItensPesquisas?caraterizacao=1 → 5025 chars\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/Itens/arrayvalores → 6322 chars\n",
      "  Fetched https://financiamento.iapmei.pt/webapi/api/Configuracao/GetRedesSociais → 190 chars\n",
      "  Extracted DOM text → 729 chars\n",
      "\n",
      "Saved 12 documents to data/dataset.json\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from playwright.async_api import async_playwright\n",
    "import nest_asyncio\n",
    "\n",
    "# ------------------------------\n",
    "# Settings\n",
    "# ------------------------------\n",
    "OUTPUT_DIR = Path(\"data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SITES = [\n",
    "    \"https://financiamento.iapmei.pt/inicio/home\",\n",
    "    \"https://transparencia.gov.pt/\",\n",
    "    # Add more websites here\n",
    "]\n",
    "\n",
    "# ------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------\n",
    "def clean_html(html_str):\n",
    "    if not html_str:\n",
    "        return \"\"\n",
    "    return BeautifulSoup(html_str, \"html.parser\").get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "def fetch_json(url, params=None):\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def extract_all_text(data):\n",
    "    \"\"\"Recursively extract all meaningful strings from JSON\"\"\"\n",
    "    texts = []\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for v in data.values():\n",
    "            texts.extend(extract_all_text(v))\n",
    "    elif isinstance(data, list):\n",
    "        for el in data:\n",
    "            texts.extend(extract_all_text(el))\n",
    "    elif isinstance(data, str) and len(data.strip()) > 20:\n",
    "        texts.append(clean_html(data))\n",
    "\n",
    "    return texts\n",
    "\n",
    "def extract_nextjs_data(html):\n",
    "    \"\"\"Extract JSON embedded in Next.js pages\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    script = soup.find(\"script\", {\"id\": \"__NEXT_DATA__\"})\n",
    "    if script and script.string:\n",
    "        try:\n",
    "            return json.loads(script.string)\n",
    "        except:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def extract_text_from_html(html):\n",
    "    \"\"\"Extract visible text from HTML as a fallback\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    texts = []\n",
    "    for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'td', 'li']):\n",
    "        t = tag.get_text(strip=True)\n",
    "        if t:\n",
    "            texts.append(t)\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "# ------------------------------\n",
    "# Capture endpoints via Playwright\n",
    "# ------------------------------\n",
    "async def capture_json_endpoints(site_url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        endpoints = set()\n",
    "        page_content = None\n",
    "\n",
    "        # Capture API responses with JSON content-type\n",
    "        async def handle_response(res):\n",
    "            try:\n",
    "                ct = res.headers.get(\"content-type\", \"\")\n",
    "                if \"application/json\" in ct.lower():\n",
    "                    endpoints.add(res.url)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        page.on(\"response\", handle_response)\n",
    "        await page.goto(site_url, wait_until=\"domcontentloaded\")\n",
    "\n",
    "        # Scroll to trigger lazy-loaded requests\n",
    "        for _ in range(3):\n",
    "            await page.mouse.wheel(0, 2000)\n",
    "            await page.wait_for_timeout(1000)\n",
    "\n",
    "        page_content = await page.content()\n",
    "        await browser.close()\n",
    "        return endpoints, page_content\n",
    "\n",
    "# ------------------------------\n",
    "# Process one website\n",
    "# ------------------------------\n",
    "async def process_site(site_url):\n",
    "    print(f\"\\nProcessing site: {site_url}\")\n",
    "    endpoints, html_content = await capture_json_endpoints(site_url)\n",
    "    print(f\"  Discovered {len(endpoints)} JSON endpoints\")\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    # 1️⃣ Fetch JSON endpoints\n",
    "    for ep in endpoints:\n",
    "        try:\n",
    "            data = fetch_json(ep)\n",
    "            text = \"\\n\".join(extract_all_text(data))\n",
    "            if text.strip():\n",
    "                dataset.append({\"url\": ep, \"text\": text})\n",
    "                print(f\"  Fetched {ep} → {len(text)} chars\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error fetching {ep}: {e}\")\n",
    "\n",
    "    # 2️⃣ Extract Next.js embedded data\n",
    "    next_data = extract_nextjs_data(html_content)\n",
    "    if next_data:\n",
    "        text = \"\\n\".join(extract_all_text(next_data))\n",
    "        if text.strip():\n",
    "            dataset.append({\"url\": site_url + \" (__NEXT_DATA__)\", \"text\": text})\n",
    "            print(f\"  Extracted Next.js data → {len(text)} chars\")\n",
    "\n",
    "    # 3️⃣ Fallback: scrape visible DOM content\n",
    "    dom_text = extract_text_from_html(html_content)\n",
    "    if dom_text.strip():\n",
    "        dataset.append({\"url\": site_url + \" (DOM)\", \"text\": dom_text})\n",
    "        print(f\"  Extracted DOM text → {len(dom_text)} chars\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# ------------------------------\n",
    "# Main function\n",
    "# ------------------------------\n",
    "async def main():\n",
    "    all_data = await asyncio.gather(*(process_site(site) for site in SITES))\n",
    "    flat_data = [item for sublist in all_data for item in sublist]\n",
    "\n",
    "    output_file = OUTPUT_DIR / \"dataset.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(flat_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved {len(flat_data)} documents to {output_file}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Run\n",
    "# ------------------------------\n",
    "nest_asyncio.apply()\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b149c-8f48-4f5a-bde4-0a8db5661470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
